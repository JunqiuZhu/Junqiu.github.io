
<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Junqiu Zhu: Research Page</title>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap-5.1.3-dist/css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/portfolio-item.css" rel="stylesheet">
    
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">

    <link rel="icon" href="imgs/jiu.png">

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-dark navbar-expand-lg fixed-top" style="background-color: #58B2DC;">
      <div class="container">
        <a class="navbar-brand" href="#">Junqiu's Research Page (Photo-realistic Rendering)</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item active">
              <a class="nav-link" href="#">Home
                <span class="sr-only">(current)</span>
              </a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="#research">Research</a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="#publications">Publications</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Content -->
    <div class="container mt-4">
	<br />
	<br />
	<br />
      <!-- <br> -->
      <!-- Portfolio Item Row -->
      <div class="row">
		
        <div class="col-md-3">
          <img class="img-fluid rounded" src="imgs/junqiu9.jpg" alt="">

        </div>

        <div class="col-md-3">
          <h3 class="my-4">Junqiu Zhu <span style="font-family: sans-serif">(朱君秋)</span>  </h3>

          
          <p>
            <i class="far fa-envelope"></i> junqiuzhu@UCSB.edu
          </p>
          <p>
            <i class="far fa-folder-open"></i>
            <a href=https://drive.google.com/file/d/1Az8C1aBXk1eduCgEmUixDPyXEg4en4Di/view?usp=sharing" target="_blank">Curriculum Vitae</a> [Nov 2023]</p>
        </div>
		<div class="col-md-6">
		<br />
		<br />
		<br />

		I'm a postdoctoral fellow of University of California Santa Babara advised by Prof. 
<a href="https://sites.cs.ucsb.edu/~lingqi/" target="_blank">Lingqi Yan</a>. I receieved my Ph.D. degree from Shandong University, China, in 2022,  working under 
the supervision of  Prof. <a href="http://vr.sdu.edu.cn/info/1010/1073.htm" target="_blank">Xiangxu Meng</a>,
co-supervised by Prof. <a href="http://vr.sdu.edu.cn/info/1010/1060.htm" target="_blank">Lu Wang</a>, 
Prof. <a href="http://vr.sdu.edu.cn/info/1010/1062.htm" target="_blank">Yanning Xu</a>. I was remotely supervised by Prof.
<a href="https://sites.cs.ucsb.edu/~lingqi/" target="_blank">Lingqi Yan</a> during my doctoral studies.
<!--           I am a Ph.D. candidate at Shandong University, China working under the supervision of  Prof. <a href="http://vr.sdu.edu.cn/info/1010/1073.htm" target="_blank">Xiangxu Meng</a>,
co-supervised by Prof. <a href="http://vr.sdu.edu.cn/info/1010/1060.htm" target="_blank">Lu Wang</a>, Prof. <a href="http://vr.sdu.edu.cn/info/1010/1062.htm" target="_blank">Yanning Xu</a>. I'm fortunate and proud of being remotely supervised by Prof.
<a href="https://sites.cs.ucsb.edu/~lingqi/" target="_blank">Lingqi Yan</a> during my doctoral studies. -->
        </div>
        
      </div>


      <a class="anchor" name="research"></a>
      <hr class="style-two">

      <h3 class="my-3">Research</h3>
<!--       <table>
      <tr>
        <td><img class="img-fluid" src="images/research_6.jpg"></td>
        <th>+</th>
        <td><img class="img-fluid" src="images/research_3.jpg"></td>
        <th>+</th>
        <td><img class="img-fluid" src="images/research_4.jpg"></td>
        <th>+</th>
        <td><img class="img-fluid" src="images/research_5.jpg"></td>
        <th>=</th>
        <td><img class="img-fluid" src="images/research_1.jpg"></td>
      </tr>
      <tr>
        <td><center>RT / offline Rendering</center></td>
        <td></td>
        <td><center>Appearance Modeling</center></td>
        <td></td>
        <td><center>Virtual / Augmented Reality</center></td>
        <td></td>
        <td><center>Machine Learning</center></td>
        <td></td>
        <td><center>Ultimate Realism</center></td>
      </tr>
      </table>
      <br>
 -->
      

      <p>My research is in Computer Graphics and mainly focuses on visual appearance modeling and rendering. I work to understand why materials in the real world look the way they do and how we can accurately and effectively model their interaction with light.</p>

      

      



      <a class="anchor" name="publications"></a>
      <hr class="style-two">




      <h3 class="my-4">My Publications </h3>
	    <div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/suozijia.jpg" height="256" width="256">
            </div>
            <div class="col-md-9">
              <p><b>A Practical and Hierarchical Yarn-based Shading Model for Cloth</b><br>
                Junqiu Zhu, Zahra Montazeri, Jean-Marie Aubry,  Ling-Qi Yan, 
Andrea Weidlich<br>
              <i>Computer Graphics Forum (Proceedings of Eurographics Symposium on Rendering 2023)  </i>
           <a href="https://sites.cs.ucsb.edu/~lingqi/publications/paper_egsr23cloth.pdf">[Paper]</a> <br> <span style="background-color: #ffffff;color: #d43535;"><b>EGSR 2023 Best Paper Award</b></span> and <span style="background-color: #ffffff;color: #d43535;"><b>EGSR 2023 Best Visual Effects Award</b></font></span> <br> 

			   Realistic cloth rendering is a longstanding challenge in computer graphics due to the intricate geometry and hierarchical structure of cloth: Fibers form plies which in turn are combined into yarns which then are woven or knitted into fabrics. Previous fiber-based models have achieved high-quality close-up rendering, but they suffer from high computational cost, which limits their practicality. In this paper, we propose a novel hierarchical model that analytically aggregates light simulation on the fiber level by building on dual-scattering theory. Based on this, we can perform an efficient simulation of ply and yarn shading. Compared to previous methods, our approach is faster and uses less memory while preserving a similar accuracy. We demonstrate both through comparison with existing fiber-based shading models. Our yarn shading model can be applied to curves or surfaces, making it highly versatile for cloth shading. This duality paired with its simplicity and flexibility makes the model particularly useful for film and games production.
		</div>
          </div>
          <br>
	 <div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/cloth.jpg" height="256" width="256">
            </div>
            <div class="col-md-9">
              <p><b>A Realistic Surface-based Cloth Rendering Model</b><br>
                Junqiu Zhu, Adrain Jarabo, Carlos Aliaga,  Ling-Qi Yan, Matt (Jen-Yuan) Chiang<br>
              <i>ACM SIGGRAPH 2023 ((Conference Track)) </i>
            <a href="https://sites.cs.ucsb.edu/~lingqi/publications/paper_sig23cloth.pdf">[Paper]</a>
		       <a href="https://sites.cs.ucsb.edu/~lingqi/publications/video_sig23cloth.mp4">[Video]</a> 
		       <a href="https://sites.cs.ucsb.edu/~lingqi/publications/supplementary_sig23cloth.pdf">[Supplementary]</a> 
		      <a href="https://www.bilibili.com/video/BV13u4y1i7oa/?spm_id_from=333.999.0.0&vd_source=9b54b92c5593c8fe760c1ee89833a5e4">[Presentation]</a> 
		      <br />
			   We propose a surface-based cloth shading model that generates realistic
cloth appearance with ply-level details. It generalizes previous surface-based
models to a broader set of cloth including knitted and thin woven cloth. Our
model takes into account the most dominant visual features of cloth, including anisotropic S-shaped reflection highlight, cross-shaped transmission
highlights, delta transmission, and shadowing masking. We model these
elements via a comprehensive micro-scale BSDF and a meso-scale effective
BSDF formulation. Then, we propose an implementation that leverages the
Monte Carlo sampler of path tracing for reducing precomputation to the bare
minimum, by evaluating the effective BSDF as a Monte Carlo estimate, and
encoding visibility using anisotropic spherical Gaussians. We demonstrate
our model by replicating a set of woven and knitted fabrics, showing good
match with respect to captured photographs
		</div>
          </div>
          <br>	    
	  
	  <div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/ham.jpg" height="208" width="256">
            </div>
            <div class="col-md-9">
              <p><b>Practical Level-of-detail Aggregation of Fur Appearance</b><br>
                Junqiu Zhu, Sizhe Zhao, Lu Wang, Yanning Xu,  Ling-Qi Yan<br>
              <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2022)</i>
              <a href="https://sites.cs.ucsb.edu/~lingqi/project_page/fur_aggregation/index.html">[Project Page]</a> <br />
			   Fur appearance rendering is crucial for the realism of computer generated imagery, but is also a challenge in computer graphics for many years. Much effort has been made to accurately simulate the multiple-scattered light transport among fur fibers, but the computation cost is still very high, since the number of fur fibers is usually extremely large. In this paper, we aim at reducing the number of fur fibers while preserving realistic fur appearance. We present an aggregated fur appearance model, using one thick cylinder to accurately describe the aggregated optical behavior of a bunch of fur fibers, including the multiple scattering of light among them. Then, to acquire the parameters of our aggregated model, we use a lightweight neural network to map individual fur fiber s optical properties to those in our aggregated model. Finally, we come up with a practical heuristic that guides the simplification process of fur dynamically at different bounces of the light, leading to a practical level-of-detail rendering scheme. Our method achieves nearly the same results as the ground truth, but performs 3.8 -13.5 times faster. 
            </div>
          </div>
          <br>	      
		      
		      
		      
	<div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/survey_1.jpg" height="170" width="256">
            </div>
            <div class="col-md-9">
              <p><b>Recent Advances in Glinty Appearance Rendering</b><br>
                Junqiu Zhu, Sizhe Zhao, Yanning Xu, Xiangxu Meng, Lu Wang, Ling-Qi Yan<br>
              <i>Computational Visual Media, 2022</i>
              <a href="https://sites.cs.ucsb.edu/~lingqi/publications/paper_glints_survey.pdf">[PDF]</a> <br />
			   The interaction between light and materials is key to physically-based realistic rendering. However, it is also complex to be analyzed, 
especially when the materials contain a large number of details and thus exhibit glinty visual effects. 
	Recent methods of glinty appearance are favored as an important component of next-generation computer graphics. 
	In this survey, we propose a comprehensive and state-of-the-art research on glinty appearance rendering. We start by presenting a 
	unified definition of glinty appearance based on the microfacet theory. Then, we summarize prior works from two aspects, including representation
	and practical rendering. Further, we implement typical methods on our unified platform and compare the performances in terms of visual effects, rendering speed,
	and memory consumption. Finally, we briefly discuss the limitations and future research directions. 
	Our analyses, implementations, and comparisons will provide insights for readers to choose the proper methods in engineering in this line of research. 
            </div>
          </div>
          <br>	      

        
			<div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/realtimeglint.jpg" height="192" width="256">
            </div>
            <div class="col-md-9">
              <p><b>Real-Time Microstructure Rendering with MIP-mapped Normal Map Samples</b><br>
                Haowen Tan*, Junqiu Zhu* (*: dual first authors), Xiangxu Meng, Yanning Xu, Lu Wang, Ling-Qi Yan<br>
              <i>Computer Graphics Forum (2022)</i>
              <a href="https://sites.cs.ucsb.edu/~lingqi/publications/paper_rtglints.pdf" target="_blank">[Paper]</a>
      <a href="https://sites.cs.ucsb.edu/~lingqi/publications/video_rtglints.mp4" target="_blank">[Video]</a> <br />
			   Normal map-based microstructure rendering method can generate both glint and scratch appearance accurately, but the extra high-resolution normal map 
		that defines every microfacet normal may incur high storage and computation costs. We present an example-based real-time rendering method 
		for arbitrary microstructure materials, which also greatly reduces required storage space. Our method takes a small-size normal map sample 
		as input. We implicitly synthesize a high-resolution normal map from the normal map sample and construct MIP-mapped position-normal 4D Gaussian 
		lobes. Based on the above MIP-mapped 4D lobes and a LUT data structure for the synthesized high-resolution normal map, an efficient Gaussian query 
		method is presented to evaluate the P-NDFs (Position-Normal Distribution Functions) for shading. We can render complex scenes with both glint and 
		scratch surfaces in real-time (&gt 30 fps) with a full high-definition resolution, and the space required for each microstructure material is decreased to 30MB.
            </div>
          </div>
          <br>
		  <div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/comlum.jpg" height="152" width="256">
            </div>
            <div class="col-md-9">
              <p><b>Neural Complex Luminaires: Representation and Rendering</b><br>
                Junqiu Zhu, Yaoyi Bai, Zilin Xu, Steve Bako, Edgar Velázquez-Armendáriz, Lu Wang, Pradeep Sen, Miloš Hašan, Ling-Qi Yan<br>
              <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2021)</i>
              <a href="https://sites.cs.ucsb.edu/~lingqi/publications/paper_complum.pdf" target="_blank">[PDF]</a>
      <a href="https://sites.cs.ucsb.edu/~lingqi/publications/video_complum.mp4" target="_blank">[Video]</a>
	  <a href="view-source:https://sites.cs.ucsb.edu/~lingqi/" target="_blank">[Code]</a>
     <br />
			  Physically-based rendering of complex luminaires, such as grand chandeliers in concert halls, 
		can be extremely costly. The emitting sources are typically encased in complex
		refractive geometry, creating difficult light paths that require many samples to 
		evaluate with Monte Carlo approaches. Previous work has attempted to speed up this process,
		but the methods are either inaccurate, require very large lightfield storage, and/or do not fit 
		well into modern path tracing frameworks. Inspired by the success of deep networks, 
		which can model complex relationships robustly and be evaluated efficiently, 
		we propose to use a machine learning framework to compress a complex luminaire's light field into an implicit neural
		representation. Our approach can easily plug into conventional renderers, as it works with the standard techniques of 
		path tracing and multiple importance sampling (MIS). Our solution is to train three networks to perform the essential operations 
		for evaluating the complex luminaire at a specific point and view direction, importance sampling a point on the luminaire given 
		a shading location, and blending to determine the transparency of luminaire queries to properly combine them with other scene elements. 
		We perform favorably relative to state-of-the-art approaches and render final images that are close to the high sample count reference with 
		only a fraction of the computation and storage costs, with no need to store the original luminaire geometry and materials.
            </div>
          </div>
          <br>
		  <div class="row vert-offset-top-1 vert-offset-bottom-1">
            <div class="col-md-3">
              <img class="img-fluid" src="imgs/svbrdf.jpg" height="192" width="256">
            </div>
            <div class="col-md-9">
              <p><b>A Stationary SVBRDF Material Modeling Method Based on Discrete Microsurface</b><br>
                 Junqiu Zhu, Yanning Xu, Lu Wang<br>
              <i>Computer Graphics Forum (Proceedings of Pacific Graphics 2019)</i>
              <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.13876" target="_blank">[PDF]</a><br> 
			    Microfacet theory is commonly used to build reflectance models for surfaces.
		While traditional microfacet‐based models assume that the distribution of a surface's microstructure is continuous, 
		recent studies indicate that some surfaces with tiny, discrete and stochastic facets exhibit glittering visual effects, 
		while some surfaces with structured features exhibit anisotropic specular reflection. Accordingly, this paper proposes an efficient
		and stationary method of surface material modeling to process both glittery and non‐glittery surfaces in a consistent way. Our method 
		comprises two steps: in the preprocessing step, we take a fixed‐size sample normal map as input, then organize 4D microfacet trees in position and 
		normal space for arbitrary‐sized surfaces; we also cluster microfacets into 4D K‐lobes via the adaptive k‐means method. In the rendering step, 
		moreover, surface normals can be efficiently evaluated using pre‐clustered microfacets. Our method is able to efficiently render any structured, 
		discrete and continuous micro‐surfaces using a precisely reconstructed surface NDF.
		Our method is both faster and uses less memory compared to the state‐of‐the‐art glittery surface modeling works.
            </div>
          </div>
          <br>

          


          

    <!-- /.container -->

    <!-- Footer -->
    <footer class="py-3 .bg-white">
      <div class="container">
        <p class="m-0 text-center text-white"></p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  </body>

</html>
